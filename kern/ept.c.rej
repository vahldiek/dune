--- kern/ept.c	2017-03-01 14:20:02.942618199 +0100
+++ kern/ept.c	2017-03-03 17:15:53.685132938 +0100
@@ -29,6 +29,8 @@
 #define EPT_LEVELS  4   /* 0 through 3 */
 #define HUGE_PAGE_SIZE  2097152
 
+#define EPTP_LIST_ENTRIES 512
+
 static inline bool cpu_has_vmx_ept_execute_only(void)
 {
     return vmx_capability.ept & VMX_EPT_EXECUTE_ONLY_BIT;
@@ -59,6 +61,25 @@
     return vmx_capability.ept & VMX_EPT_PAGE_WALK_4_BIT;
 }
 
+static inline bool cpu_has_vmx_ept_ad_bits(void)
+{
+    return vmx_capability.ept & VMX_EPT_AD_BIT;
+}
+
+static u64 construct_eptp(unsigned long root_hpa)
+{
+    u64 eptp;
+
+    /* TODO write the value reading from MSR */
+    eptp = VMX_EPT_DEFAULT_MT |
+        VMX_EPT_DEFAULT_GAW << VMX_EPT_GAW_EPTP_SHIFT;
+    if (cpu_has_vmx_ept_ad_bits())
+        eptp |= VMX_EPT_AD_ENABLE_BIT;
+    eptp |= (root_hpa & PAGE_MASK);
+
+    return eptp;
+}
+
 #define VMX_EPT_FAULT_READ  0x01
 #define VMX_EPT_FAULT_WRITE 0x02
 #define VMX_EPT_FAULT_INS   0x04
@@ -164,15 +185,29 @@
         return ADDR_INVAL;
 }
 
+inline unsigned eptp_to_ept_index(struct vmx_vcpu *vcpu, unsigned long eptp)
+{
+    unsigned i;
+    for (i = 0; i < vcpu->num_epts; i++)
+        if (vcpu->eptp_list[i] == eptp)
+            return i;
+
+    return (unsigned)-1;
+}
+
+inline unsigned vmx_get_current_ept_index(struct vmx_vcpu *vcpu)
+{
+    return eptp_to_ept_index(vcpu, vcpu->eptp);
+}
+
 #define ADDR_TO_IDX(la, n) \
     ((((unsigned long) (la)) >> (12 + 9 * (n))) & ((1 << 9) - 1))
 
-static int
-ept_lookup_gpa(struct vmx_vcpu *vcpu, void *gpa, int level,
-       int create, epte_t **epte_out)
+static int ept_lookup_gpa(struct vmx_vcpu *vcpu, unsigned ept_index, void *gpa,
+        int level, int create, epte_t **epte_out)
 {
+    epte_t *dir = (epte_t *) vcpu->ept_root_list[ept_index];
     int i;
-    epte_t *dir = (epte_t *) __va(vcpu->ept_root);
 
     for (i = EPT_LEVELS - 1; i > level; i--) {
         int idx = ADDR_TO_IDX(gpa, i);
@@ -207,7 +242,7 @@
 }
 
 static int
-ept_lookup(struct vmx_vcpu *vcpu, struct mm_struct *mm,
+ept_lookup(struct vmx_vcpu *vcpu, unsigned ept_index, struct mm_struct *mm,
        void *hva, int level, int create, epte_t **epte_out)
 {
     void *gpa = (void *) hva_to_gpa(vcpu, mm, (unsigned long) hva);
@@ -219,7 +254,7 @@
         return -EINVAL;
     }
 
-    return ept_lookup_gpa(vcpu, gpa, level, create, epte_out);
+    return ept_lookup_gpa(vcpu, ept_index, gpa, level, create, epte_out);
 }
 
 static void free_ept_page(epte_t epte)
@@ -235,9 +270,9 @@
     put_page(page);
 }
 
-static void vmx_free_ept(unsigned long ept_root)
+static void vmx_free_ept(void *ept_root)
 {
-    epte_t *pgd = (epte_t *) __va(ept_root);
+    epte_t *pgd = (epte_t *) ept_root;
     int i, j, k, l;
 
     for (i = 0; i < PTRS_PER_PGD; i++) {
@@ -348,8 +383,8 @@
     return 1;
 }
 
-static int ept_set_pfnmap_epte(struct vmx_vcpu *vcpu, int make_write,
-                unsigned long gpa, unsigned long hva)
+static int ept_set_pfnmap_epte(struct vmx_vcpu *vcpu, unsigned ept_index, int
+        make_write, unsigned long gpa, unsigned long hva)
 {
     struct vm_area_struct *vma;
     struct mm_struct *mm = current->mm;
@@ -378,7 +413,7 @@
 
     /* NOTE: pfn's can not be huge pages, which is quite a relief here */
     spin_lock(&vcpu->ept_lock);
-    ret = ept_lookup_gpa(vcpu, (void *) gpa, 0, 1, &epte);
+    ret = ept_lookup_gpa(vcpu, ept_index, (void *) gpa, 0, 1, &epte);
     if (ret) {
         spin_unlock(&vcpu->ept_lock);
         printk(KERN_ERR "ept: failed to lookup EPT entry\n");
@@ -405,8 +440,8 @@
     return 0;
 }
 
-static int ept_set_epte(struct vmx_vcpu *vcpu, int make_write,
-            unsigned long gpa, unsigned long hva)
+static int ept_set_epte(struct vmx_vcpu *vcpu, unsigned ept_index,
+        int make_write, unsigned long gpa, unsigned long hva)
 {
     int ret;
     epte_t *epte, flags;
@@ -416,7 +451,7 @@
 
     ret = get_user_pages_fast(hva, 1, make_write, &page);
     if (ret != 1) {
-        ret = ept_set_pfnmap_epte(vcpu, make_write, gpa, hva);
+        ret = ept_set_pfnmap_epte(vcpu, ept_index, make_write, gpa, hva);
         if (ret)
             printk(KERN_ERR "ept: failed to get user page %lx\n", hva);
         return ret;
@@ -431,7 +466,7 @@
     else if (huge_shift == 21)
         level = 1;
 
-    ret = ept_lookup_gpa(vcpu, (void *) gpa,
+    ret = ept_lookup_gpa(vcpu, ept_index, (void *) gpa,
                  level, 1, &epte);
     if (ret) {
         spin_unlock(&vcpu->ept_lock);
@@ -479,7 +514,7 @@
 int vmx_do_ept_fault(struct vmx_vcpu *vcpu, unsigned long gpa,
              unsigned long gva, int fault_flags)
 {
-    int ret;
+    unsigned ept_index;
     unsigned long hva = gpa_to_hva(vcpu, current->mm, gpa);
     int make_write = (fault_flags & VMX_EPT_FAULT_WRITE) ? 1 : 0;
 
@@ -490,10 +525,8 @@
 
     pr_debug("ept: GPA: 0x%lx, GVA: 0x%lx, HVA: 0x%lx, flags: %x\n",
          gpa, gva, hva, fault_flags);
-
-    ret = ept_set_epte(vcpu, make_write, gpa, hva);
-
-    return ret;
+    ept_index = vmx_get_current_ept_index(vcpu);
+    return ept_set_epte(vcpu, ept_index, make_write, gpa, hva);
 }
 
 /**
@@ -504,7 +537,7 @@
  *
  * Returns 1 if the page was removed, 0 otherwise
  */
-static int ept_invalidate_page(struct vmx_vcpu *vcpu,
+static int ept_invalidate_page(struct vmx_vcpu *vcpu, unsigned ept_index,
                    struct mm_struct *mm,
                    unsigned long addr)
 {
@@ -518,7 +551,7 @@
     }
 
     spin_lock(&vcpu->ept_lock);
-    ret = ept_lookup_gpa(vcpu, (void *) gpa, 0, 0, &epte);
+    ret = ept_lookup_gpa(vcpu, ept_index, (void *) gpa, 0, 0, &epte);
     if (ret) {
         spin_unlock(&vcpu->ept_lock);
         return 0;
@@ -541,7 +574,7 @@
  *
  * Returns 1 if the page is mapped, 0 otherwise
  */
-static int ept_check_page_mapped(struct vmx_vcpu *vcpu,
+static int ept_check_page_mapped(struct vmx_vcpu *vcpu, unsigned ept_index,
                  struct mm_struct *mm,
                  unsigned long addr)
 {
@@ -555,7 +588,7 @@
     }
 
     spin_lock(&vcpu->ept_lock);
-    ret = ept_lookup_gpa(vcpu, (void *) gpa, 0, 0, &epte);
+    ret = ept_lookup_gpa(vcpu, ept_index, (void *) gpa, 0, 0, &epte);
     spin_unlock(&vcpu->ept_lock);
 
     return !ret;
@@ -570,7 +603,7 @@
  *
  * Returns 1 if the page was accessed, 0 otherwise
  */
-static int ept_check_page_accessed(struct vmx_vcpu *vcpu,
+static int ept_check_page_accessed(struct vmx_vcpu *vcpu, unsigned ept_index,
                    struct mm_struct *mm,
                    unsigned long addr,
                    bool flush)
@@ -585,7 +618,7 @@
     }
 
     spin_lock(&vcpu->ept_lock);
-    ret = ept_lookup_gpa(vcpu, (void *) gpa, 0, 0, &epte);
+    ret = ept_lookup_gpa(vcpu, ept_index, (void *) gpa, 0, 0, &epte);
     if (ret) {
         spin_unlock(&vcpu->ept_lock);
         return 0;
@@ -612,10 +645,12 @@
                          unsigned long address)
 {
     struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);
+    unsigned ept_index;
 
     pr_debug("ept: invalidate_page addr %lx\n", address);
 
-    ept_invalidate_page(vcpu, mm, address);
+    for (ept_index = 0; ept_index < vcpu->num_epts; ept_index++)
+        ept_invalidate_page(vcpu, ept_index, mm, address);
 }
 
 static void ept_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
@@ -626,14 +661,18 @@
     struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);
     int ret;
     epte_t *epte;
-    unsigned long pos = start;
+    unsigned long pos;
     bool sync_needed = false;
+    unsigned ept_index;
 
     pr_debug("ept: invalidate_range_start start %lx end %lx\n", start, end);
 
+    for (ept_index = 0; ept_index < vcpu->num_epts; ept_index++)
+    {
+        pos = start;
     spin_lock(&vcpu->ept_lock);
     while (pos < end) {
-        ret = ept_lookup(vcpu, mm, (void *) pos, 0, 0, &epte);
+            ret = ept_lookup(vcpu, ept_index, mm, (void *) pos, 0, 0, &epte);
         if (!ret) {
             pos += epte_big(*epte) ? HUGE_PAGE_SIZE : PAGE_SIZE;
             ept_clear_epte(epte);
@@ -643,6 +682,7 @@
     }
     spin_unlock(&vcpu->ept_lock);
 
+    }
     if (sync_needed)
         vmx_ept_sync_vcpu(vcpu);
 }
@@ -660,6 +700,7 @@
                     pte_t pte)
 {
     struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);
+    unsigned ept_index;
 
     pr_debug("ept: change_pte addr %lx flags %lx\n", address, pte_flags(pte));
 
@@ -669,18 +710,22 @@
      * get_user_pages_fast(). As a result, we just invalidate the
      * page so that the mapping can be recreated later during a fault.
      */
-    ept_invalidate_page(vcpu, mm, address);
+    for (ept_index = 0; ept_index < vcpu->num_epts; ept_index++)
+        ept_invalidate_page(vcpu, ept_index, mm, address);
 }
 
 static int ept_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,
                           struct mm_struct *mm,
-                          unsigned long address
+                          unsigned long address,
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3,18,0)
-                          , unsigned long end_addr
+                          unsigned long end_addr
 #endif
+
         )
 {
     struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);
+    unsigned ept_index;
+    int ret = 0;
 
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(3,18,0)
     pr_debug("ept: clear_flush_young addr %lx end %lx\n", address, end_addr);
@@ -688,10 +733,14 @@
     pr_debug("ept: clear_flush_young addr %lx\n", address);
 #endif
 
+    for (ept_index = 0; ept_index < vcpu->num_epts; ept_index++)
+    {
     if (!vcpu->ept_ad_enabled)
-        return ept_invalidate_page(vcpu, mm, address);
+            ret &= ept_invalidate_page(vcpu, ept_index, mm, address);
     else
-        return ept_check_page_accessed(vcpu, mm, address, true);
+            ret &= ept_check_page_accessed(vcpu, ept_index, mm, address, true);
+    }
+    return ret;
 }
 
 static int ept_mmu_notifier_test_young(struct mmu_notifier *mn,
@@ -699,13 +748,19 @@
                        unsigned long address)
 {
     struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);
+    unsigned ept_index;
+    int ret = 0;
 
     pr_debug("ept: test_young addr %lx\n", address);
 
+    for (ept_index = 0; ept_index < vcpu->num_epts; ept_index++)
+    {
     if (!vcpu->ept_ad_enabled)
-        return ept_check_page_mapped(vcpu, mm, address);
+            ret &= ept_check_page_mapped(vcpu, ept_index, mm, address);
     else
-        return ept_check_page_accessed(vcpu, mm, address, false);
+            ret &= ept_check_page_accessed(vcpu, ept_index, mm, address, false);
+    }
+    return ret;
 }
 
 static void ept_mmu_notifier_release(struct mmu_notifier *mn,
@@ -723,21 +778,21 @@
     .release        = ept_mmu_notifier_release,
 };
 
-int vmx_init_ept(struct vmx_vcpu *vcpu)
+void *vmx_alloc_ept(void)
 {
     void *page = (void *) __get_free_page(GFP_KERNEL);
 
     if (!page)
-        return -ENOMEM;
+        return NULL;
 
     memset(page, 0, PAGE_SIZE);
-    vcpu->ept_root =  __pa(page);
 
-    return 0;
+    return page;
 }
 
 int vmx_create_ept(struct vmx_vcpu *vcpu)
 {
+    unsigned ept_index;
     int ret;
 
     vcpu->mmu_notifier.ops = &ept_mmu_notifier_ops;
@@ -748,13 +803,61 @@
     return 0;
 
 fail:
-    vmx_free_ept(vcpu->ept_root);
+    for (ept_index = 0; ept_index < vcpu->num_epts; ept_index++)
+        vmx_free_ept(vcpu->ept_root_list[ept_index]);
 
     return ret;
 }
 
 void vmx_destroy_ept(struct vmx_vcpu *vcpu)
 {
+    unsigned ept_index;
     mmu_notifier_unregister(&vcpu->mmu_notifier, current->mm);
-    vmx_free_ept(vcpu->ept_root);
+    for (ept_index = 0; ept_index < vcpu->num_epts; ept_index++)
+        vmx_free_ept(vcpu->ept_root_list[ept_index]);
+}
+
+int vmx_init_eptp_list(struct vmx_vcpu *vcpu)
+{
+    unsigned long *eptp_list;
+    void **ept_root_list;
+    int i;
+
+    eptp_list = kmalloc(sizeof(unsigned long) * EPTP_LIST_ENTRIES, GFP_KERNEL);
+    if (!eptp_list)
+        return -ENOMEM;
+
+    ept_root_list = kmalloc(sizeof(void *) * EPTP_LIST_ENTRIES, GFP_KERNEL);
+    if (!ept_root_list)
+    {
+        kfree(eptp_list);
+        return -ENOMEM;
+    }
+
+    vcpu->eptp_list = eptp_list;
+    vcpu->ept_root_list = ept_root_list;
+    memset(vcpu->eptp_list, 0, sizeof(unsigned long) * EPTP_LIST_ENTRIES);
+    memset(vcpu->ept_root_list, 0, sizeof(void *) * EPTP_LIST_ENTRIES);
+
+    vcpu->num_epts = 3;
+    for (i = 0; i < vcpu->num_epts; i++)
+    {
+        vcpu->ept_root_list[i] = vmx_alloc_ept();
+        if (!vcpu->ept_root_list[i])
+        {
+            kfree(eptp_list);
+            kfree(ept_root_list);
+            /* TODO free succesfull ept allocs so far. */
+            return -ENOMEM;
+        }
+        vcpu->eptp_list[i] = construct_eptp(__pa(vcpu->ept_root_list[i]));
+    }
+
+    return 0;
+}
+
+void vmx_free_eptp_list(struct vmx_vcpu *vcpu)
+{
+    kfree(vcpu->eptp_list);
+    kfree(vcpu->ept_root_list);
 }
